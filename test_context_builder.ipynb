{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5ef334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from graph_rag_query import GraphRAGQuery\n",
    "from graph_rag_embeddings import EntityEmbeddings\n",
    "from graph_rag_hybrid import HybridRetriever\n",
    "from graph_rag_context import ContextBuilder\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b88f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Neo4j connection\n",
    "NEO4J_URI = \"neo4j+s://0c367113.databases.neo4j.io\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"gTO1K567hBLzkRdUAhhEb-UqvBjz0i3ckV3M9v_-Nio\"\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD\n",
    ")\n",
    "\n",
    "print(\"K·∫øt n·ªëi Neo4j th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510d0db",
   "metadata": {},
   "source": [
    "## Step 1: Initialize All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea3485f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components t·ª´ c√°c b∆∞·ªõc tr∆∞·ªõc\n",
    "print(\"Loading components...\")\n",
    "\n",
    "# B∆∞·ªõc 1: Graph Queries\n",
    "graph_query = GraphRAGQuery(graph)\n",
    "print(\"Graph queries ready\")\n",
    "\n",
    "# B∆∞·ªõc 2: Embeddings\n",
    "embeddings = EntityEmbeddings(graph)\n",
    "print(\"Embeddings ready\")\n",
    "\n",
    "# B∆∞·ªõc 3: Hybrid Retriever\n",
    "hybrid = HybridRetriever(graph_query, embeddings)\n",
    "print(\"Hybrid retriever ready\")\n",
    "\n",
    "# B∆∞·ªõc 4: Context Builder\n",
    "builder = ContextBuilder(max_context_length=8000)\n",
    "print(\"Context builder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b380b",
   "metadata": {},
   "source": [
    "## Step 2: Test Basic Context Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a105a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve context cho c√¢u h·ªèi\n",
    "question = \"Ai l√† ng∆∞·ªùi ch·ªâ huy chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "# Step 1: Hybrid retrieval\n",
    "retrieval_context = hybrid.retrieve(\n",
    "    question=question,\n",
    "    top_k=10,\n",
    "    vector_top_k=5,\n",
    "    expansion_depth=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build structured context\n",
    "context = builder.build_rag_context(\n",
    "    question=question,\n",
    "    retrieval_context=retrieval_context,\n",
    "    max_entities=10,\n",
    "    max_relationships=15\n",
    ")\n",
    "\n",
    "print(\"=== STRUCTURED CONTEXT ===\")\n",
    "print(f\"Question: {context['question']}\")\n",
    "print(f\"Question type: {context['question_type']}\")\n",
    "print(f\"Entities: {context['entity_count']}\")\n",
    "print(f\"Relationships: {context['relationship_count']}\")\n",
    "print(f\"Summary: {context['context_summary']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b0b895",
   "metadata": {},
   "source": [
    "## Step 3: Format for Gemini - Q&A Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format cho Q&A task\n",
    "qa_prompt = builder.format_for_gemini(\n",
    "    context=context,\n",
    "    prompt_type=\"qa\",\n",
    "    include_instructions=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GEMINI PROMPT - Q&A STYLE\")\n",
    "print(\"=\" * 70)\n",
    "print(qa_prompt)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"Prompt length: {len(qa_prompt)} characters\")\n",
    "print(f\"Estimated tokens: {builder.estimate_token_count(qa_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded9ffbd",
   "metadata": {},
   "source": [
    "## Step 4: Test Different Prompt Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Summary prompt\n",
    "summary_question = \"T√≥m t·∫Øt v·ªÅ chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß\"\n",
    "\n",
    "summary_retrieval = hybrid.retrieve(summary_question, top_k=10)\n",
    "summary_context = builder.build_rag_context(summary_question, summary_retrieval)\n",
    "\n",
    "summary_prompt = builder.format_for_gemini(\n",
    "    context=summary_context,\n",
    "    prompt_type=\"summary\",\n",
    "    include_instructions=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GEMINI PROMPT - SUMMARY STYLE\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Explanation prompt\n",
    "explain_question = \"Gi·∫£i th√≠ch vai tr√≤ c·ªßa V√µ Nguy√™n Gi√°p trong kh√°ng chi·∫øn\"\n",
    "\n",
    "explain_retrieval = hybrid.retrieve(explain_question, top_k=12, expansion_depth=2)\n",
    "explain_context = builder.build_rag_context(explain_question, explain_retrieval)\n",
    "\n",
    "explain_prompt = builder.format_for_gemini(\n",
    "    context=explain_context,\n",
    "    prompt_type=\"explain\",\n",
    "    include_instructions=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GEMINI PROMPT - EXPLANATION STYLE\")\n",
    "print(\"=\" * 70)\n",
    "print(explain_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73d53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Timeline prompt\n",
    "timeline_question = \"C√°c s·ª± ki·ªán quan tr·ªçng t·ª´ 1945 ƒë·∫øn 1954\"\n",
    "\n",
    "timeline_retrieval = hybrid.retrieve(timeline_question, top_k=15)\n",
    "timeline_context = builder.build_rag_context(timeline_question, timeline_retrieval)\n",
    "\n",
    "timeline_prompt = builder.format_for_gemini(\n",
    "    context=timeline_context,\n",
    "    prompt_type=\"timeline\",\n",
    "    include_instructions=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GEMINI PROMPT - TIMELINE STYLE\")\n",
    "print(\"=\" * 70)\n",
    "print(timeline_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1075418",
   "metadata": {},
   "source": [
    "## Step 5: Test Token Limit Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v·ªõi large context\n",
    "large_question = \"L·ªãch s·ª≠ kh√°ng chi·∫øn ch·ªëng Ph√°p t·ª´ 1945 ƒë·∫øn 1954\"\n",
    "\n",
    "large_retrieval = hybrid.retrieve(\n",
    "    large_question, \n",
    "    top_k=20,\n",
    "    expansion_depth=2,\n",
    "    include_paths=True\n",
    ")\n",
    "\n",
    "large_context = builder.build_rag_context(\n",
    "    large_question,\n",
    "    large_retrieval,\n",
    "    max_entities=20,\n",
    "    max_relationships=30\n",
    ")\n",
    "\n",
    "# Format v√† check size\n",
    "large_prompt = builder.format_for_gemini(large_context, prompt_type=\"qa\")\n",
    "\n",
    "print(\"=== LARGE CONTEXT ===\")\n",
    "print(f\"Entities: {large_context['entity_count']}\")\n",
    "print(f\"Relationships: {large_context['relationship_count']}\")\n",
    "print(f\"Prompt length: {len(large_prompt)} characters\")\n",
    "print(f\"Estimated tokens: {builder.estimate_token_count(large_prompt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate n·∫øu qu√° d√†i\n",
    "MAX_TOKENS = 6000  # Gemini free tier: 8K context, ƒë·ªÉ 6K cho safe\n",
    "\n",
    "if builder.estimate_token_count(large_prompt) > MAX_TOKENS:\n",
    "    print(f\"‚ö†Ô∏è Context qu√° d√†i! Truncating...\\n\")\n",
    "    \n",
    "    truncated_context = builder.truncate_to_token_limit(\n",
    "        large_context,\n",
    "        max_tokens=MAX_TOKENS\n",
    "    )\n",
    "    \n",
    "    truncated_prompt = builder.format_for_gemini(truncated_context, prompt_type=\"qa\")\n",
    "    \n",
    "    print(\"=== AFTER TRUNCATION ===\")\n",
    "    print(f\"Entities: {truncated_context['entity_count']}\")\n",
    "    print(f\"Relationships: {truncated_context['relationship_count']}\")\n",
    "    print(f\"Prompt length: {len(truncated_prompt)} characters\")\n",
    "    print(f\"Estimated tokens: {builder.estimate_token_count(truncated_prompt)}\")\n",
    "else:\n",
    "    print(\"‚úÖ Context size OK!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dad17e",
   "metadata": {},
   "source": [
    "## Step 6: Test Multi-Turn Conversation Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate conversation history\n",
    "conversation_history = [\n",
    "    {\n",
    "        \"question\": \"Ai l√† l√£nh ƒë·∫°o kh√°ng chi·∫øn ch·ªëng Ph√°p?\",\n",
    "        \"answer\": \"H·ªì Ch√≠ Minh l√† ng∆∞·ªùi l√£nh ƒë·∫°o ch√≠nh c·ªßa phong tr√†o kh√°ng chi·∫øn ch·ªëng Ph√°p, v·ªõi vai tr√≤ Ch·ªß t·ªãch n∆∞·ªõc v√† l√£nh t·ª• c·ªßa ƒê·∫£ng C·ªông s·∫£n Vi·ªát Nam.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"V√µ Nguy√™n Gi√°p ch·ªâ huy nh·ªØng chi·∫øn d·ªãch n√†o?\",\n",
    "        \"answer\": \"ƒê·∫°i t∆∞·ªõng V√µ Nguy√™n Gi√°p ch·ªâ huy nhi·ªÅu chi·∫øn d·ªãch quan tr·ªçng, ƒë·∫∑c bi·ªát l√† Chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß nƒÉm 1954.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Current question (follow-up)\n",
    "followup_question = \"Chi·∫øn d·ªãch ƒë√≥ di·ªÖn ra nh∆∞ th·∫ø n√†o?\"\n",
    "\n",
    "# Retrieve context\n",
    "followup_retrieval = hybrid.retrieve(followup_question, top_k=10)\n",
    "followup_context = builder.build_rag_context(followup_question, followup_retrieval)\n",
    "\n",
    "# Create multi-turn prompt\n",
    "multi_turn_prompt = builder.create_multi_turn_context(\n",
    "    conversation_history=conversation_history,\n",
    "    current_context=followup_context,\n",
    "    max_history=2\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MULTI-TURN CONVERSATION PROMPT\")\n",
    "print(\"=\" * 70)\n",
    "print(multi_turn_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997af07b",
   "metadata": {},
   "source": [
    "## Step 7: Test Few-Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define few-shot examples\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Ai l√† Ch·ªß t·ªãch n∆∞·ªõc Vi·ªát Nam D√¢n ch·ªß C·ªông h√≤a?\",\n",
    "        \"answer\": \"H·ªì Ch√≠ Minh l√† Ch·ªß t·ªãch n∆∞·ªõc Vi·ªát Nam D√¢n ch·ªß C·ªông h√≤a, ƒë·ªìng th·ªùi l√† ng∆∞·ªùi s√°ng l·∫≠p v√† l√£nh ƒë·∫°o ƒê·∫£ng C·ªông s·∫£n Vi·ªát Nam.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß k·∫øt th√∫c khi n√†o?\",\n",
    "        \"answer\": \"Chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß k·∫øt th√∫c v√†o ng√†y 7/5/1954, ƒë√°nh d·∫•u th·∫Øng l·ª£i quy·∫øt ƒë·ªãnh c·ªßa qu√¢n ƒë·ªôi Vi·ªát Nam trong kh√°ng chi·∫øn ch·ªëng Ph√°p.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Base prompt\n",
    "test_question = \"Vi·ªát Minh ƒë∆∞·ª£c th√†nh l·∫≠p nƒÉm n√†o?\"\n",
    "test_retrieval = hybrid.retrieve(test_question, top_k=8)\n",
    "test_context = builder.build_rag_context(test_question, test_retrieval)\n",
    "base_prompt = builder.format_for_gemini(test_context, prompt_type=\"qa\")\n",
    "\n",
    "# Add examples\n",
    "few_shot_prompt = builder.add_examples_to_prompt(\n",
    "    base_prompt=base_prompt,\n",
    "    examples=examples,\n",
    "    max_examples=2\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEW-SHOT PROMPT\")\n",
    "print(\"=\" * 70)\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e627fa",
   "metadata": {},
   "source": [
    "## Step 8: Compare Prompt Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645352af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different prompt formats cho c√πng question\n",
    "compare_question = \"T·∫ßm quan tr·ªçng c·ªßa chi·∫øn th·∫Øng ƒêi·ªán Bi√™n Ph·ªß\"\n",
    "\n",
    "compare_retrieval = hybrid.retrieve(compare_question, top_k=10, expansion_depth=2)\n",
    "compare_context = builder.build_rag_context(compare_question, compare_retrieval)\n",
    "\n",
    "prompt_types = [\"qa\", \"summary\", \"explain\"]\n",
    "\n",
    "for ptype in prompt_types:\n",
    "    prompt = builder.format_for_gemini(compare_context, prompt_type=ptype, include_instructions=False)\n",
    "    tokens = builder.estimate_token_count(prompt)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROMPT TYPE: {ptype.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Length: {len(prompt)} chars\")\n",
    "    print(f\"Tokens: ~{tokens}\")\n",
    "    print(f\"\\nPreview (first 500 chars):\")\n",
    "    print(prompt[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08619c88",
   "metadata": {},
   "source": [
    "## Step 9: Test Context Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze context quality\n",
    "def analyze_context_quality(context: dict) -> dict:\n",
    "    \"\"\"Analyze v√† report context quality metrics\"\"\"\n",
    "    \n",
    "    entities = context['entities']\n",
    "    relationships = context['relationships']\n",
    "    \n",
    "    # Entity diversity\n",
    "    entity_types = {}\n",
    "    for e in entities:\n",
    "        etype = e['type']\n",
    "        entity_types[etype] = entity_types.get(etype, 0) + 1\n",
    "    \n",
    "    # Average relevance\n",
    "    avg_relevance = sum(e['relevance_score'] for e in entities) / len(entities) if entities else 0\n",
    "    \n",
    "    # Description coverage\n",
    "    with_desc = sum(1 for e in entities if e['description'])\n",
    "    desc_coverage = (with_desc / len(entities) * 100) if entities else 0\n",
    "    \n",
    "    # Relationship density\n",
    "    rel_density = len(relationships) / len(entities) if entities else 0\n",
    "    \n",
    "    return {\n",
    "        'entity_count': len(entities),\n",
    "        'entity_types': entity_types,\n",
    "        'avg_relevance': avg_relevance,\n",
    "        'description_coverage': desc_coverage,\n",
    "        'relationship_count': len(relationships),\n",
    "        'relationship_density': rel_density\n",
    "    }\n",
    "\n",
    "# Test v·ªõi different questions\n",
    "test_questions = [\n",
    "    \"Ai ch·ªâ huy qu√¢n ƒë·ªôi?\",\n",
    "    \"Chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß di·ªÖn ra ·ªü ƒë√¢u?\",\n",
    "    \"C√°c s·ª± ki·ªán nƒÉm 1954\"\n",
    "]\n",
    "\n",
    "print(\"=== CONTEXT QUALITY ANALYSIS ===\")\n",
    "for question in test_questions:\n",
    "    retrieval = hybrid.retrieve(question, top_k=10)\n",
    "    context = builder.build_rag_context(question, retrieval)\n",
    "    quality = analyze_context_quality(context)\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"  Entities: {quality['entity_count']}\")\n",
    "    print(f\"  Avg relevance: {quality['avg_relevance']:.3f}\")\n",
    "    print(f\"  Description coverage: {quality['description_coverage']:.1f}%\")\n",
    "    print(f\"  Relationships: {quality['relationship_count']}\")\n",
    "    print(f\"  Rel. density: {quality['relationship_density']:.2f}\")\n",
    "    print(f\"  Entity types: {quality['entity_types']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143d6c4",
   "metadata": {},
   "source": [
    "## Step 10: Save Sample Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a991a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample prompts ƒë·ªÉ reference\n",
    "sample_prompts = {}\n",
    "\n",
    "sample_questions = [\n",
    "    (\"Ai l√£nh ƒë·∫°o kh√°ng chi·∫øn ch·ªëng Ph√°p?\", \"qa\"),\n",
    "    (\"T√≥m t·∫Øt chi·∫øn d·ªãch ƒêi·ªán Bi√™n Ph·ªß\", \"summary\"),\n",
    "    (\"Gi·∫£i th√≠ch vai tr√≤ c·ªßa V√µ Nguy√™n Gi√°p\", \"explain\"),\n",
    "]\n",
    "\n",
    "for question, ptype in sample_questions:\n",
    "    retrieval = hybrid.retrieve(question, top_k=10)\n",
    "    context = builder.build_rag_context(question, retrieval)\n",
    "    prompt = builder.format_for_gemini(context, prompt_type=ptype)\n",
    "    \n",
    "    sample_prompts[f\"{ptype}_{question[:30]}\"] = {\n",
    "        'question': question,\n",
    "        'type': ptype,\n",
    "        'prompt': prompt,\n",
    "        'token_estimate': builder.estimate_token_count(prompt)\n",
    "    }\n",
    "\n",
    "# Save to file\n",
    "with open('sample_prompts.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_prompts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"‚úÖ Saved sample prompts to sample_prompts.json\")\n",
    "print(f\"\\nSummary:\")\n",
    "for key, data in sample_prompts.items():\n",
    "    print(f\"  {data['type']:10s} - {data['question'][:40]:40s} - ~{data['token_estimate']} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc80b5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **B∆∞·ªõc 4 ho√†n th√†nh!**\n",
    "\n",
    "**ƒê√£ implement:**\n",
    "1. ‚úÖ `build_rag_context()` - Structure retrieval results\n",
    "2. ‚úÖ 4 prompt types cho Gemini:\n",
    "   - **Q&A**: Tr·∫£ l·ªùi c√¢u h·ªèi tr·ª±c ti·∫øp\n",
    "   - **Summary**: T√≥m t·∫Øt th√¥ng tin\n",
    "   - **Explain**: Gi·∫£i th√≠ch chi ti·∫øt\n",
    "   - **Timeline**: X√¢y d·ª±ng timeline s·ª± ki·ªán\n",
    "3. ‚úÖ Token management:\n",
    "   - Estimate token count\n",
    "   - Auto truncate n·∫øu qu√° limit\n",
    "4. ‚úÖ Multi-turn conversation support\n",
    "5. ‚úÖ Few-shot examples integration\n",
    "6. ‚úÖ Context quality analysis\n",
    "\n",
    "**Prompt Format:**\n",
    "- Instructions ‚Üí Context (Entities + Relationships + Paths) ‚Üí Question ‚Üí Answer\n",
    "- Structured v√† d·ªÖ ƒë·ªçc cho Gemini\n",
    "- Ti·∫øng Vi·ªát t·ª± nhi√™n\n",
    "- Source citations (n·∫øu c√≥)\n",
    "\n",
    "**Token Limits:**\n",
    "- Target: ~4000-6000 tokens cho context\n",
    "- Gemini free: 8K context window\n",
    "- Auto truncate n·∫øu exceed\n",
    "\n",
    "**Next:** B∆∞·ªõc 5 & 6 - RAG Prompts + LLM Integration (connect v·ªõi Gemini API) üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
